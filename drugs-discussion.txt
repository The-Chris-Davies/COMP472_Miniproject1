    As should be expected, the repeatability of the classifiers falls into two groups: one, where the accuracy, macro F1 and weighted F1 scores all have a negligible deviation, and one, where all three deviate significantly. These are separated based on the inclusion of randomness in the initialization of the model and the size of the model's latent space.
    The models with negligible deviation include the naive bayes, decison trees, and the perceptron. The naive bayes and decision tree classifiers are both fully deterministic, as in the initialization and training of these models does not depend on any random variable. The Perceptron is different, in that it is initialized with random values for its weights, but because it can only perform a linear classification, the latent space of the model is very simple, and so the model consistently reaches the same minimum (even though this is not a particularly useful result, considering the low metrics across the board).
    The multi-layer perceptrons, however, have high std. deviations for all their metrics. This is due to the random values used to initialize these models, and the much larger latent space available to them. Because of this, these models are much more likely to converge to separate local minima, and thus display different behaviour between instances.
    I performed task 2 with different values of max_iter for the MLP classifiers - I hope this is acceptable, as the value of max_iter was not specified. With the default of 200 iterations, none of the models had the chance to converge, and so the deviation was very large (around 0.2) and the accuracy was relatively low (accuracy around 0.6) compared to what is expected from the model. When run with 1000 maximum iterations, the model still did not converge, and the deviation was greater than before (around 0.3), but the accuracy was greatly improved. With 10000 maximum iterations, the models converged with much better metrics. The grid search MLP, with a much better hidden topology, had a deviation in accuracy and weighted F1 around 0.02, and a macro F1 around twice that. The 1x100 MLP was not much more repeatable (accuracy and weighted F1 ~0.2), but still had the same pattern of the macro F1 score deviating much more. This is expected, as the model sees drugs A, B, and C much less frequently, so will misclassify them more often.
